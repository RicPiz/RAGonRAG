1.  Traditional relational databases scale poorly for vector operations essential for semantic search with millions or billions of documents. Vector databases are optimized to store and search huge quantities of vector data and implement vector-oriented algorithms, making them significantly faster and more efficient for RAG.
2.  KNN search requires calculating vector distances between the prompt and every document, leading to linear scaling issues (e.g., billions of calculations for billions of documents). ANN algorithms use clever data structures, like proximity graphs, to enable significantly faster searches by approximating the nearest neighbors.
3.  A hierarchical proximity graph in HNSW consists of multiple layers, with higher layers containing exponentially fewer vectors. Searches begin in the top layer, making large jumps to quickly narrow down the approximate neighborhood, then progressively move to lower, denser layers to refine the search, resulting in logarithmic search times.
4.  The primary purpose of chunking is to break longer text documents into smaller, more manageable pieces. This is beneficial because many embedding models have text limits, it improves search relevancy by allowing more specific context to be captured, and it ensures only the most relevant text is sent to the LLM.
5.  Overlapping chunks ensure that words at the boundaries of one chunk will also appear with surrounding context in an adjacent chunk. This minimizes instances where important words or cohesive thoughts are split, thereby preserving context and often positively impacting search relevancy.
6.  Semantic chunking works by vectorizing a growing chunk and the following sentence, then splitting the chunk when their semantic distance (dissimilarity) crosses a threshold. Its main advantage is creating variably sized chunks that align with the author's train of thought, keeping related concepts together more effectively than fixed-size splits.
7.  Query rewriting is the process of transforming a user's conversational prompt into a more optimized search query. An LLM typically facilitates this by clarifying ambiguities, adding domain-specific terminology or synonyms, and removing unnecessary information, guided by a specific prompt.
8.  A bi-encoder embeds documents and prompts separately into single vectors, allowing for pre-computation of document embeddings for speed. A cross-encoder, however, concatenates the prompt and document text and processes them together to directly output a relevancy score, capturing deeper contextual interactions.
9.  The primary trade-off when using ColBERT compared to a standard bi-encoder is significantly increased vector storage memory footprint. ColBERT requires storing a separate dense vector for each token in a document and prompt, whereas a bi-encoder stores only a single dense vector per document.
10. Re-ranking is a post-retrieval process that re-scores and re-orders an initial set of documents returned by the vector database. More computationally expensive models (like cross-encoders) can be used here because they only need to process a small subset of documents (e.g., 20-100), rather than the entire knowledge base, making their cost and latency acceptable for a significant quality boost.