1.  The Retriever's role is to find and prepare useful information. In contrast, the LLM's primary role is to actually use that retrieved information to generate a high-quality, coherent, and relevant response to the user's prompt. It acts as the "brains" that synthesizes and formulates the final output.
2.  The encoder processes the original text to develop a deep contextual understanding of its meaning, while the decoder uses this understanding to generate new text. Most modern LLMs only include the second decoder component, as their primary function is text generation.
3.  The attention mechanism allows each token in a prompt to look at every other token, understanding their meaning and position. It then decides which other tokens it should "pay the most attention to," thereby developing a very detailed representation of the relationships and context between all tokens in the text.
4.  Greedy decoding instructs the LLM to always pick the token with the highest probability, eliminating randomness. An advantage is deterministic output, making it predictable for tasks like code completion. A disadvantage is that it can lead to predictable, generic, or repetitive text, as there's no mechanism to break out of loops.
5.  Adjusting the "temperature" parameter changes the shape of the probability distribution for next token generation. A lower temperature leads to a spikier distribution, making the LLM more confident and deterministic, while a higher temperature flattens the distribution, giving unlikely tokens more chance and resulting in more varied or creative text.
6.  Three quantifiable metrics to consider when choosing an LLM are: Model size (number of parameters), Cost (price per million tokens, often with input/output differences), and Context window (maximum number of tokens an LLM can process). Other valid answers include Time to first token/Speed and Training cutoff date.
7.  Top-K sampling limits the LLM to choosing from the k most likely tokens, regardless of their probabilities. Top-P sampling, conversely, limits the LLM to choosing tokens whose cumulative probability falls below a specified threshold, making it more dynamic as the pool of selectable tokens changes based on the distribution's flatness.
8.  A system prompt provides the LLM with high-level instructions to influence its overall behavior, tone, and procedures. It sets the foundational guidelines, such as instructing the LLM to use only retrieved documents, cite sources, or adopt a specific personality, thereby ensuring consistent and controlled responses.
9.  LLMs hallucinate because they are designed to produce probable text sequences, not necessarily factually accurate ones; they prioritize plausibility. A core strategy in RAG systems to mitigate this is to ground the LLM's responses in the information retrieved from a trusted knowledge base, often enforced through system prompt instructions.
10. In agentic RAG, specialized LLMs improve performance by focusing on single, simpler tasks within a larger workflow (e.g., routing, evaluating, citing). Using different LLMs for different steps allows for optimization, as lightweight, fast, and cheap models can handle simple tasks, while larger, more capable models handle complex generation, aligning model strengths with workflow needs.