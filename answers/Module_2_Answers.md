1.  The primary objective of a retriever is to find relevant documents in a knowledge base that can help an LLM respond to a user prompt. This task is challenging because user prompts are often natural language conversations rather than structured queries, and the knowledge base can contain diverse, unstructured information.
2.  Metadata filtering uses rigid criteria (e.g., author, date, access privileges) associated with documents to narrow down search results. A key advantage is its ability to strictly include or exclude documents based on definitive rules, while a significant limitation is that it ignores the actual content of the document and cannot rank results on its own.
3.  The "bag of words" concept treats text as an unordered collection of words, where only the presence and frequency of each word matter. Its main implication for keyword search is that the order of words in a prompt or document is completely ignored, focusing solely on individual word counts and matches.
4.  TF-IDF ranks documents based on keyword frequency within a document (TF) and the rarity of those keywords across the knowledge base (IDF). BM25 improves upon TF-IDF by introducing term frequency saturation (diminishing returns for repeated keywords) and diminishing penalties for document length, making it more flexible and generally better performing.
5.  An embedding model enables semantic search by mapping pieces of text (words, sentences, documents) to high-dimensional vectors. These vectors have the special property that texts with similar meanings are embedded to locations that are mathematically "close" to one another in vector space.
6.  Euclidean distance measures the shortest straight-line distance between two vectors in space, similar to geometry. Cosine similarity, on the other hand, measures the similarity in the direction of two vectors, regardless of their magnitude, making it particularly effective in high-dimensional spaces where all points tend to be far apart.
7.  Contrastive training is a technique used to train embedding models by providing them with positive pairs (similar texts) and negative pairs (dissimilar texts). The model iteratively adjusts its internal parameters to pull positive pairs closer together in vector space and push negative pairs farther apart, thereby learning semantic relationships.
8.  A hybrid search pipeline first performs parallel keyword and semantic searches, generating two ranked lists. These lists are then filtered using metadata. Finally, a combination algorithm like Reciprocal Rank Fusion merges the two filtered lists into a single, final ranked list, from which the top K documents are returned.
9.  Precision measures the proportion of retrieved documents that are actually relevant, indicating how trustworthy the results are. Recall measures the proportion of all relevant documents in the knowledge base that were successfully retrieved, indicating how comprehensive the search was. These often present a trade-off: increasing the number of retrieved documents might increase recall but decrease precision if more irrelevant documents are included.
10. Mean Reciprocal Rank (MRR) measures the average reciprocal of the rank of the first relevant document found across multiple searches. It is particularly useful because it emphasizes the importance of a retriever placing at least one relevant document as high as possible in its ranking, reflecting how quickly a user might find a useful result.
