1. The primary purpose of RAG is to improve the quality and accuracy of an LLM's response by giving it access to additional information beyond its initial training data. This allows the LLM to answer questions using facts drawn from proprietary, recent, or highly specialized data.
2. RAG addresses this limitation by acting as an intermediary step where a retriever finds relevant information from a knowledge base. This retrieved information is then inserted directly into the prompt given to the LLM, effectively "teaching" the LLM new facts for that specific query.
3. Similar to a person, an LLM with RAG first performs a "retrieval" phase, collecting necessary information from a knowledge base. After gathering this context, it enters the "generation" phase, reasoning over the provided information to formulate its response.
4. Hallucinations occur when LLMs generate responses that sound plausible but are not factually true, often due to a lack of relevant training data. RAG mitigates this by "grounding" the LLM's responses in specific, retrieved documents, making them less likely to invent information.
5. The retriever component in a RAG system is responsible for managing a knowledge base of trusted information and, upon receiving a user prompt, finding and retrieving the most relevant documents or pieces of information from that knowledge base. It then passes this information to the LLM.
6. RAG allows LLMs to stay up-to-date by simply updating the information within the knowledge base, similar to updating entries in any other database. This is far more efficient than the costly and time-consuming process of retraining the entire LLM.
7. An augmented prompt is a modified prompt that a RAG system sends to the LLM. It combines the user's original question with the relevant information retrieved from the knowledge base, providing the LLM with the necessary context to generate an accurate response.
8. Personalized assistants (e.g., in email clients or text messages) might use small knowledge bases of personal information like contact lists or specific document folders. This provides the LLM with dense, important context, enabling it to complete tasks with significantly higher relevance to the user's specific needs.
9. An LLM's context window is the maximum length of text (tokens) it can process at one time. This is a practical limitation for RAG because longer augmented prompts, containing more retrieved information, require more computation and can eventually exceed the model's capacity, leading to errors or truncation.
10. Agentic RAG differs from earlier systems by giving an AI agent the autonomy to decide what information to retrieve and how to do it, rather than relying on human-engineered rules. This allows the system to perform multi-step retrieval, decide on keywords for web searches, or query specialized databases as needed, making it more flexible and powerful.