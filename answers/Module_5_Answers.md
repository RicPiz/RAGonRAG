1. Production challenges for a RAG system fall into several categories. These include scaling issues like reduced throughput and increased latency, memory, and compute costs; the unpredictability of user prompts; messy, fragmented, or non-text real-world data; and security and privacy concerns. Ultimately, mistakes in production have real business impacts, making robust monitoring and evaluation critical.
2. The two dimensions of the RAG evaluation framework are scope and evaluator type. Scope determines whether an evaluation targets an individual component (like the retriever) or the overall system. Evaluator type asks how the evaluation is generated, with the main types being code-based, "LLM as a judge," or human feedback.
3. "LLM as a judge" is an evaluation technique that uses a language model to grade components of a RAG system's performance, such as the relevance of retrieved documents. Its main advantage is that it is more flexible than code-based evaluations and cheaper than human feedback. Its disadvantages are that models can have biases (e.g., favoring responses from their own model family) and require clear, discrete rubrics to perform well.
4. A trace allows an observer to follow a single prompt's entire path through the RAG pipeline. It shows how the prompt is modified by each component, such as the query sent to the retriever, the chunks returned, the re-ranker's output, and the final prompt sent to the LLM. This is highly useful for debugging and identifying the source of errors for poorly performing responses.
5. Quantization is a compression technique that replaces the high-precision model weights in an LLM or the values in an embedding vector with a lower-precision data type (e.g., from 32-bit floats to 8-bit integers). The fundamental trade-off is a significant reduction in memory footprint, cost, and latency in exchange for a typically small sacrifice in retrieval relevance or response quality.
6. Matryoshka embedding models, named after Russian nesting dolls, are designed so their dimensions are sorted by how information-dense they are. This allows a user to select only a subset of the dimensions (e.g., the first 100 out of 1000) for tasks like initial retrieval, which saves space and speeds up computation. The full vector can then be used for more detailed tasks like rescoring.
7. The two largest costs are the vector database and the large language models. To manage LLM costs, one can use smaller or quantized models and limit the number of input/output tokens. To manage vector database costs, one can strategically use different tiers of memory (RAM, disk, cloud object storage), ensuring only the most critical data, like the HNSW index, is kept in the most expensive storage.
8. Multi-tenancy is an approach where documents in a vector database are divided by the user or organization they belong to, with each tenant having their own HNSW index. This helps manage costs by allowing the system to load a specific tenant's data into expensive, fast memory (like RAM) only when it is actively needed. It also enhances security by ensuring a user can only access documents associated with their specific role or access level.
9. The biggest culprit for latency is running a transformer, making large language model calls the primary source. An effective strategy to reduce this latency is to use a smaller or quantized language model, as they will always run faster on the same hardware. Another approach is to use a smaller router LLM to direct simple queries to fast models and complex queries to slower, more powerful models.
10. Multimodal RAG systems use a multimodal embedding model that can embed different data types, like text and images, into the same vector space. This model places items with similar meanings (e.g., an image of a dog and the word "dog") close to each other in the vector space. Retrieval then works as normal, where an embedded prompt (text or image) is used to find the closest text or image vectors in the knowledge base.