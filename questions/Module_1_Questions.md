1. What is the primary purpose of Retrieval Augmented Generation (RAG) for Large Language Models (LLMs)?
2. How does RAG address the limitation of an LLM's knowledge being restricted to its training data?
3. Explain the two main phases of how an LLM with RAG would answer a complex question, using an analogy from the text if you wish.
4. What are "hallucinations" in the context of LLMs, and how does RAG help to mitigate them?
5. Describe the function of the "retriever" component within a RAG system.
6. How does RAG allow LLMs to stay up-to-date with rapidly changing information without costly retraining?
7. What is an "augmented prompt" in a RAG system, and what information does it typically contain?
8. In what scenarios might a RAG system use a relatively small, personalized knowledge base, and what benefit does this provide?
9. Explain the concept of an LLM's "context window" and why it is a practical limitation for RAG systems.
10. What is "agentic RAG," and how does it differ from earlier generations of RAG systems?