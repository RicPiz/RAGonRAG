1.  What is the primary role of the LLM in a RAG system, as opposed to the Retriever?
2.  Briefly describe the purpose of the encoder and decoder components in the original transformer architecture. Which component do most modern LLMs primarily use?
3.  How does the attention mechanism in a transformer architecture enhance the LLM's understanding of a prompt?
4.  Explain the concept of "greedy decoding" in LLMs and provide one advantage and one disadvantage of using it.
5.  What is the effect of adjusting the "temperature" parameter in an LLM's token generation? How does a lower temperature differ from a higher one?
6.  List three quantifiable metrics you would consider when choosing an LLM for a RAG project, besides quality.
7.  Describe the main difference between Top-K sampling and Top-P sampling.
8.  What is the purpose of a "system prompt" in prompt engineering, and how does it influence LLM behavior?
9.  Why do LLMs hallucinate, and what is one core strategy a RAG system employs to mitigate this?
10. In the context of agentic RAG, how do specialized LLMs contribute to improving system performance, and why might you use different LLMs for different steps?